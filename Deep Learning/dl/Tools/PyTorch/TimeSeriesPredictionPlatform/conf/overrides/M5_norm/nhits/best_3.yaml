trainer:
  config:
    batch_size: 16384
    ema: False
    num_epochs: 30
  
  optimizer:
    lr: 0.003778126865168513
  
  criterion:
    _target_: torch.nn.L1Loss

model:
  config:
    activation: Tanh
    pooling_mode: MaxPool1d
    hidden_size: 2048
    n_blocks: [1,2,2]
    n_freq_downsample: [6,2,1]
    n_pool_kernel_size: [3,3,1]
    n_mlp_layers: 3

dataset:
  config:
    memory_mapped: True 
    train_samples: 1000000

evaluator:
  config:
    save_predictions: True
    batch_size: 16384
